{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: alphalens==0.3.2 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: nltk==3.8.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (3.8)\n",
      "Requirement already satisfied: numpy==1.24.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (1.24.0)\n",
      "Requirement already satisfied: ratelimit==2.2.1 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: requests==2.28.1 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (2.28.1)\n",
      "Requirement already satisfied: scikit-learn==1.2 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: tqdm==4.64.1 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (4.64.1)\n",
      "Collecting bs4==0.0.1\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (1.9.3)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (0.13.5)\n",
      "Requirement already satisfied: IPython>=3.2.3 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (8.7.0)\n",
      "Requirement already satisfied: pandas>=0.18.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: seaborn>=0.6.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (3.6.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from nltk==3.8.0->-r requirements.txt (line 2)) (2022.10.31)\n",
      "Requirement already satisfied: click in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from nltk==3.8.0->-r requirements.txt (line 2)) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from nltk==3.8.0->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from requests==2.28.1->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from requests==2.28.1->-r requirements.txt (line 5)) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from requests==2.28.1->-r requirements.txt (line 5)) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from requests==2.28.1->-r requirements.txt (line 5)) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from scikit-learn==1.2->-r requirements.txt (line 6)) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from tqdm==4.64.1->-r requirements.txt (line 8)) (0.4.4)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (5.8.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (3.0.36)\n",
      "Requirement already satisfied: stack-data in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.1.6)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (2.13.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from jedi>=0.16->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (9.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (22.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from pandas>=0.18.0->alphalens==0.3.2->-r requirements.txt (line 1)) (2022.7)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.11->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from statsmodels>=0.6.1->alphalens==0.3.2->-r requirements.txt (line 1)) (0.5.3)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from stack-data->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from stack-data->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\mrhea\\miniconda3\\lib\\site-packages (from stack-data->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.2.2)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=9ae4e1ff7a336d3f59d636fc6b66a9a05f73172d83a701872ed1311d711a4539\n",
      "  Stored in directory: c:\\users\\mrhea\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.11.1 bs4-0.0.1 soupsieve-2.3.2.post1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the stuff we just installed from Cell one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import project_helper\n",
    "import project_tests\n",
    "import bs4\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add two corpora from nltk package stopwords removes stopwaords and wordnet lemmatizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mrhea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mrhea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 10Ks\n",
    "\n",
    "To get all the 10ks for this project we'll use their CIKs to lookup the documents from the SEC. We will load them in the next code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_lookup = {\n",
    "    'AMZN': '0001018724',\n",
    "    'BMY': '0000014272',   \n",
    "    'CNP': '0001130310',\n",
    "    'CVX': '0000093410',\n",
    "    'FL': '0000850209',\n",
    "    'FRT': '0000034903',\n",
    "    'HON': '0000773840'}\n",
    "\n",
    "additional_cik = {\n",
    "    'AEP': '0000004904',\n",
    "    'AXP': '0000004962',\n",
    "    'BA': '0000012927', \n",
    "    'BK': '0001390777',\n",
    "    'CAT': '0000018230',\n",
    "    'DE': '0000315189',\n",
    "    'DIS': '0001001039',\n",
    "    'DTE': '0000936340',\n",
    "    'ED': '0001047862',\n",
    "    'EMR': '0000032604',\n",
    "    'ETN': '0001551182',\n",
    "    'GE': '0000040545',\n",
    "    'IBM': '0000051143',\n",
    "    'IP': '0000051434',\n",
    "    'JNJ': '0000200406',\n",
    "    'KO': '0000021344',\n",
    "    'LLY': '0000059478',\n",
    "    'MCD': '0000063908',\n",
    "    'MO': '0000764180',\n",
    "    'MRK': '0000310158',\n",
    "    'MRO': '0000101778',\n",
    "    'PCG': '0001004980',\n",
    "    'PEP': '0000077476',\n",
    "    'PFE': '0000078003',\n",
    "    'PG': '0000080424',\n",
    "    'PNR': '0000077360',\n",
    "    'SYY': '0000096021',\n",
    "    'TXN': '0000097476',\n",
    "    'UTX': '0000101829',\n",
    "    'WFC': '0000072971',\n",
    "    'WMT': '0000104169',\n",
    "    'WY': '0000106535',\n",
    "    'XOM': '0000034088'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these are loaded we will pull up classes from our helper document that helps cache SEC data and avoid going over the SEC call limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_api = project_helper.SecAPI()\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_sec_data(cik, doc_type, start=0, count=60):\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    sec_data = sec_api.get(rss_url)\n",
    "    feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed\n",
    "    entries = [\n",
    "        (\n",
    "            entry.content.find('filing-href').getText(),\n",
    "            entry.content.find('filing-type').getText(),\n",
    "            entry.content.find('filing-date').getText())\n",
    "        for entry in feed.find_all('entry', recursive=False)]\n",
    "\n",
    "    return entries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take an example ticker from Amazon or AMZN and check that our links work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode characters in position 2599-2601: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m sec_data \u001b[39m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m ticker, cik \u001b[39min\u001b[39;00m cik_lookup\u001b[39m.\u001b[39mitems():\n\u001b[1;32m----> 5\u001b[0m     sec_data[ticker] \u001b[39m=\u001b[39m get_sec_data(cik, \u001b[39m'\u001b[39;49m\u001b[39m10-K\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m pprint\u001b[39m.\u001b[39mpprint(sec_data[example_ticker][:\u001b[39m5\u001b[39m])\n",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m, in \u001b[0;36mget_sec_data\u001b[1;34m(cik, doc_type, start, count)\u001b[0m\n\u001b[0;32m      6\u001b[0m rss_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.sec.gov/cgi-bin/browse-edgar?action=getcompany\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m&CIK=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m&type=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m&start=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m&count=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m&owner=exclude&output=atom\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m      8\u001b[0m     \u001b[39m.\u001b[39mformat(cik, doc_type, start, count)\n\u001b[0;32m      9\u001b[0m sec_data \u001b[39m=\u001b[39m sec_api\u001b[39m.\u001b[39mget(rss_url)\n\u001b[1;32m---> 10\u001b[0m feed \u001b[39m=\u001b[39m BeautifulSoup(sec_data\u001b[39m.\u001b[39;49mencode(\u001b[39m'\u001b[39;49m\u001b[39mascii\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m'\u001b[39m\u001b[39mxml\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfeed\n\u001b[0;32m     11\u001b[0m entries \u001b[39m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     (\n\u001b[0;32m     13\u001b[0m         entry\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mfiling-href\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mgetText(),\n\u001b[0;32m     14\u001b[0m         entry\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mfiling-type\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mgetText(),\n\u001b[0;32m     15\u001b[0m         entry\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mfiling-date\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mgetText())\n\u001b[0;32m     16\u001b[0m     \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m feed\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mentry\u001b[39m\u001b[39m'\u001b[39m, recursive\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)]\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m entries\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 2599-2601: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "example_ticker = 'AMZN'\n",
    "sec_data = {}\n",
    "\n",
    "for ticker, cik in cik_lookup.items():\n",
    "    sec_data[ticker] = get_sec_data(cik, '10-K')\n",
    "\n",
    "pprint.pprint(sec_data[example_ticker][:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b871a3ce7614c1af4ac0f6f3038671765b55e870e6ea7410304e5d31d25c1a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
